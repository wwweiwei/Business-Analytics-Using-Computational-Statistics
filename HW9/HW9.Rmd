---
title: "BACS HW (Week11)"
author: "106070038"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

# Question 1 Model fit is often determined by R2 so let’s dig into what this perspective of model fit is all about.

## a. Let’s dig into what regression is doing to compute model fit:
i. Plot Scenario 2, storing the returned points

```{r }
plot_regr_rsq <- function(points) {
  max_x <- 50
  if (nrow(points) == 0) {
    plot(NA, xlim=c(-5,max_x), ylim=c(-5,max_x), xlab="x", ylab="y")
    return()
  }
  plot(points, xlim=c(-5,max_x), ylim=c(-5,max_x), pch=19, cex=2, col="gray")
  if (nrow(points) < 2) return()
  
  mean_x <- mean(points$x)
  mean_y <- mean(points$y)
  segments(0, mean_y, max_x, mean_y, lwd=1, col="lightgray", lty="dotted")
  segments(mean_x, 0, mean_x, mean_y, lwd=1, col="lightgray", lty="dotted")
  regr <- lm(points$y ~ points$x)
  abline(regr, lwd=2, col="cornflowerblue")
  
  regr_summary <- summary(regr)
  ssr <- sum((regr$fitted.values - mean(points$y))^2)
  sse <- sum((points$y - regr$fitted.values)^2)
  sst <- sum((points$y - mean(points$y))^2)
  
  par(family="mono")
  legend("topleft", legend = c(
    paste(" Raw intercept: ", round(regr$coefficients[1], 2), "\n",
          "Raw slope    : ", round(regr$coefficients[2], 2), "\n",
          "Correlation  : ", round(cor(points$x, points$y), 2), "\n",
          "SSR          : ", round(ssr, 2), "\n",
          "SSE          : ", round(sse, 2), "\n",
          "SST          : ", round(sst, 2), "\n",
          "R-squared    : ", round(regr_summary$r.squared, 2))),
    bty="n")
  par(family="sans")
}

interactive_regression_rsq <- function(points=data.frame()) {
  cat("Click on the plot to create data points; hit [esc] to stop")
  repeat {
    plot_regr_rsq(points)
    click_loc <- locator(1)
    if (is.null(click_loc)) break
    if(nrow(points) == 0 ) {
      points <- data.frame(x=click_loc$x, y=click_loc$y)
    } else {
      points <- rbind(points, c(click_loc$x, click_loc$y))
    }
  }
  return(points)
}

# pts <- interactive_regression_rsq()
# save(pts, file='/Users/nkust/Desktop/2021Spring_Courses/BACS/HW9/pts.Rda')
```

![pts plot](/Users/nkust/Desktop/2021Spring_Courses/BACS/HW9/image_1)

ii. Run a linear model of x and y points to confirm the R2 value reported by the simulation
```{r }
load('/Users/nkust/Desktop/2021Spring_Courses/BACS/HW9/pts.Rda')
regr <- lm(y ~ x, data=pts)
summary(regr)
```

iii. Add line segments to the plot to show the regression residuals (errors)
![pts plot with line segments](/Users/nkust/Desktop/2021Spring_Courses/BACS/HW9/image_2)

```{r }
y_hat <- regr$fitted.values
# segments(pts$x, pts$y, pts$x, y_hat, col="red", lty="dotted")
```

iv. Use only pts$x, pts$y, y_hat and mean(pts$y) to compute SSE, SSR and SST, and verify R2 
```{r }
SSE <- sum((y_hat - mean(pts$y))^2)
SSE
SSR <- sum((pts$y - y_hat)^2)
SSR
SST <- SSE + SSR
SST
R_square <- 1 - (SSR/(SSE + SSR))
R_square
## verify
summary(regr)$r.square 
```

## b.Comparing scenarios 1 and 2, which do we expect to have a stronger R2 ?
- Ans: Scenario 1, because Scenarios 1 is more intensive and close to the line than Scenario 2.

## c. Comparing scenarios 3 and 4, which do we expect to have a stronger R2 ?
- Ans: Scenario 3, because Scenarios 3 is more intensive and close to the line than Scenario 4.

## d. Comparing scenarios 1 and 2, which do we expect has bigger/smaller SSE, SSR, and SST? (do not compute SSE/SSR/SST here – just provide your intuition)
- Ans:
  - SSE: Scenario 1 < Scenario 2 <br>
  - SSR: Scenario 1 > Scenario 2 <br>
  - SST: Scenario 1 $\approx$ Scenario 2 <br>

## e. Comparing scenarios 3 and 4, which do we expect has bigger/smaller SSE, SSR, and SST? (do not compute SSE/SSR/SST here – just provide your intuition)
- Ans:
  - SSE: Scenario 3 < Scenario 4 <br>
  - SSR: Scenario 3 < Scenario 4 <br>
  - SST: Scenario 3 < Scenario 4 <br>

# Question 2 We’re going to take a look back at the early heady days of global car manufacturing, when American, Japanese, and European cars competed to rule the world. Take a look at the data set in file auto-data.txt. We are interested in explaining what kind of cars have higher fuel efficiency (mpg).

```{r }
auto <- read.table("auto-data.txt", header=FALSE, na.strings = "?")
names(auto) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name")
```

## a.
i. Visualize the data in any way you feel relevant
```{r fig.height = 6, fig.width = 6}
summary(auto)
plot(auto)
```

ii. Report a correlation table of all variables, rounding to two decimal places
```{r }
cor <- cor(auto[-9], use="pairwise.complete.obs")
round(cor,2)
```

iii. From the visualizations and correlations, which variables seem to relate to mpg? <br>
```{r }
cor(auto[1], auto[-9], use="pairwise.complete.obs")
```
- Ans: displacement(-0.8042028) and weight(-0.8317409) <br>

iv. Which relationships might not be linear? (don’t worry about linearity for rest of this HW) <br>

- Ans: Based on the correlation plot, most of the relationships with model_year are not linear.<br>

v. Are there any pairs of independent variables that are highly correlated (r > 0.7)? <br>

- Ans: cylinders-displacement(0.95), cylinders-horsepower(0.84), cylinders-weight(0.90),  displacement-horsepower(0.90), displacement-weight(0.93), horsepower-weight(0.86) <br>


## b. Let’s create a linear regression model where mpg is dependent upon all other suitable variables (Note: origin is categorical with three levels, so use factor(origin) in lm(...)  to split it into two dummy variables)
i. Which independent variables have a ‘significant’ relationship with mpg at 1% significance?
```{r }
auto$origin <- factor(auto$origin)
auto <- auto[-9]
regr <- lm(auto$mpg ~ auto$cylinders + auto$displacement + auto$horsepower + auto$weight + auto$acceleration + auto$model_year + auto$origin , data = auto) 
summary(regr)
```

- Ans: displacement <br>

ii. Looking at the coefficients, is it possible to determine which independent variables are the most effective at increasing mpg? If so, which ones, and if not, why not? (hint: units!) <br>

- Ans: model_year, because the Estimate of model_year is biggest(7.770e-01)<br>

## c.Let’s try to resolve some of the issues with our regression model above

i. Create fully standardized regression results: are these slopes easier to compare?<br>

```{r }
auto_std <- data.frame(scale(auto[-8]))
auto_std_regr <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model_year , data = auto_std) 
summary(auto_std_regr)
```

ii. Regress mpg over each nonsignificant independent variable, individually.
Which ones become significant when we regress mpg over them individually? <br>
- nonsignificant independent variable: cylinders, displacement, horsepower, acceleration
```{r }
auto_std_regr_cylinders <- lm(mpg ~ cylinders , data = auto_std) 
summary(auto_std_regr_cylinders)
auto_std_regr_displacement <- lm(mpg ~ displacement , data = auto_std) 
summary(auto_std_regr_displacement)
auto_std_regr_horsepower <- lm(mpg ~ horsepower , data = auto_std) 
summary(auto_std_regr_horsepower)
auto_std_regr_acceleration <- lm(mpg ~ acceleration , data = auto_std) 
summary(auto_std_regr_acceleration)
```
- Ans: all become significant when we regress mpg over them individually (cylinders, displacement, horsepower, acceleration) <br>

iii. Plot the density of the residuals: are they normally distributed and centered around zero? <br>

```{r fig.height = 4, fig.width = 6}
plot(density(regr$residuals), main = "Density plot of residuals")
abline(v = mean(regr$residuals), col="red")
```

- Ans: not normally distributed, but it is centered around zero